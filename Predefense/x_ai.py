# -*- coding: utf-8 -*-
"""X-AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N9JG7fK23gM24Il9GKzMKv7MkRyX22bX
"""

!pip install tensorflow keras matplotlib seaborn opencv-python scikit-learn

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import cv2



from google.colab import drive
drive.mount('/content/drive')

import os
dataset_path = '/content/drive/MyDrive/Defense/Leveled_dataSet'
save_folder = '/content/drive/MyDrive/Defense/Saved_Models'
os.makedirs(save_folder, exist_ok=True)

img_size = 224
batch_size = 32

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2     # 80% train | 20% test
)

train_data = datagen.flow_from_directory(
    dataset_path,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode="categorical",
    subset="training",
    shuffle=True
)

test_data = datagen.flow_from_directory(
    dataset_path,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode="categorical",
    subset="validation",
    shuffle=False
)

base_model = DenseNet121(
    weights="imagenet",
    include_top=False,
    input_shape=(img_size, img_size, 3)
)

base_model.trainable = False   # freeze base layers first

x = GlobalAveragePooling2D()(base_model.output)
x = Dropout(0.4)(x)
output = Dense(train_data.num_classes, activation="softmax")(x)

model = Model(inputs=base_model.input, outputs=output)

model.compile(
    optimizer=Adam(1e-4),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

history = model.fit(
    train_data,
    validation_data=test_data,
    epochs=20
)



model_path = os.path.join(save_folder, "densenet121_coconut.h5")
model.save(model_path)

print("Model saved at:", model_path)

loss, acc = model.evaluate(test_data)
print("Test Accuracy:", acc)
print("Test Loss:", loss)

plt.figure(figsize=(14,5))

# accuracy
plt.subplot(1,2,1)
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.title("Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend(["Train", "Test"])

# loss
plt.subplot(1,2,2)
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(["Train", "Test"])

plt.show()

y_true = test_data.classes
y_pred = np.argmax(model.predict(test_data), axis=1)

labels = list(train_data.class_indices.keys())

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

print(classification_report(y_true, y_pred, target_names=labels))

from tensorflow.keras.models import load_model

model = load_model('/content/drive/MyDrive/Defense/Saved_Models/densenet121_coconut.h5')

def grad_cam(model, img_path, layer_name="conv5_block16_concat"):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    img_resized = cv2.resize(img, (img_size, img_size))
    img_array = img_resized.astype("float32") / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    grad_model = tf.keras.models.Model(
        [model.input],
        [
            model.get_layer(layer_name).output,
            model.output
        ]
    )

    with tf.GradientTape() as tape:
        conv_out, preds = grad_model(img_array)
        pred_index = tf.argmax(preds[0])
        pred_output = preds[:, pred_index]

    grads = tape.gradient(pred_output, conv_out)[0]
    conv_out = conv_out[0]

    weights = np.mean(grads, axis=(0, 1))
    heatmap = np.dot(conv_out, weights)

    heatmap = np.maximum(heatmap, 0)
    heatmap /= (heatmap.max() + 1e-9)

    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

    overlay = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)

    return overlay, heatmap

image_path = dataset_path + "/Bud_Rot/BudRootDropping281.jpg"

overlay, heatmap = grad_cam(model, image_path)

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.imshow(heatmap[..., ::-1])
plt.title("Grad-CAM Heatmap")

plt.subplot(1,2,2)
plt.imshow(overlay)
plt.title("Grad-CAM Overlay")
plt.show()

y_true = test_data.classes
y_pred = np.argmax(model.predict(test_data), axis=1)

labels = list(train_data.class_indices.keys())

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

print(classification_report(y_true, y_pred, target_names=labels))